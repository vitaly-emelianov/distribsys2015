Distributed Systems in Python (fall 2015)

1. Реализовать протокол с уведомлениями о доставке поверх UDP. Более конкретно, нужно реализовать класс reliable_socket, реализующий методы sendto() и recvfrom() аналогично классу socket.socket. sendto() посылает сообщение и блокируется до получения подтверждения или истечения таймаута. recvfrom() получает сообщение и отсылает подтверждение. Методы используют следующий протокол общения:
Первый байт - 0 для пакета с данными, 1 для пакета с подтверждением;
затем 4 байта уникальный ID пакета;
затем содержимое (если пакет с данными) или ничего (если подтверждение).

2. Реализовать консольный клиент к Яндекс.Диску. Клиент выполняет команду, переданную ему через командную строку. В случае ошибки пишет причину (нет доступа, файл отсутствует, и т.п.). Способ передачи авторизационного токена выбирайте сами; это может быть переменная окружения, конфиг, еще один параметр командной строки и т.п. Пример запуска скрипта:
$ python disk-client.py mv /file1.txt /file2.txt
Эта команда должна переименовать file1.txt в корне Яндекс.Диска в file2.txt.
Список команд, которые надо поддержать: ls, rm, mv, cp.
Дополнительное задание (на повышенный балл): реализовать команды upload и download, которые копируют файл с локальной файловой системы на Диск и обратно.
 
3. Написать реплицированное хранилище пар ключ-значение. Оно будет состоять из сервера-координатора и нескольких рабочих серверов, способных выполнять запросы пользователя. Пользователь отправляет запросы на основной сервер (master), который в свою очередь дублирует их на запасной (backup). В каждый момент времени запасной сервер может быть только один. Пользователь не может отправлять запросы напрямую на запасной сервер. Все взаимодействие между серверами будет происходить с помощью XMLRPC. Задача разбита на 2 задания: задание 3 - написать координатор и задание 4 - написать рабочий сервер. Теперь подробнее про координатор. Задача координатора - поддерживать актуальную картину работы сервиса: какие сервера живы, какие отвалились, кто выполняет функции master, кто backup. Серверов может быть более двух, координатор должен про них помнить и при необходимости заменять упавший сервер свободным.
Текущая картина называется view и имеет некоторый уникальный номер. При любых изменениях номер увеличивается на 1. Изменения могут быть следующие: если упал master, то его может заменить backup. Если упал backup, то его может заменить любой свободный сервер.
Сервера сообщают о себе с помощью удаленного вызова метода ping(), в который передается номер view и имя пославшего сервера. Сразу после старта сервер начинает посылать пинги с view=0. В процессе работы посылка пинга с view=n означает, что сервер подтверждает свою роль в этом view. Например, master подтверждает view, в котором есть backup, только после того, как убедится, что все данные успешно скопированы.
Сервера не сохраняют свое состояние на персистентном хранилище. Если какой-то из серверов перезагрузился, он теряет все данные и начинает заново слать пинги с view=0.
Для упрощения архитектуры мы вводим правило, что координатор не может изменять view, пока он не подтвержден текущим master. Вам нужно реализовать класс Coordinator, отвечающий за логику координатора. Публичный интерфейс этого класса содержит методы ping(number, name), master() и tick(), а также константу deadPings. ping() вызывается рабочим сервером и возвращает информацию о текущем view (это должен быть объект, содержащий поля number, master и backup). master() вызывается клиентом и возвращает имя текущего основного сервера. tick() вызывается таймером и отмечает течение времени. Если от сервера не поступало пингов в течение deadPings тиков, сервер считается упавшим.
В разделе resources есть файл test_coordinator.py, имитирующий основные ситуации, с которыми должен справляться координатор. Используйте его для отладки. Имейте в виду, что список тестов не исчерпывающий, какие-то краевые случаи могут быть не учтены.

4. Продолжаем работать над хранилищем пар ключ-значение. Общую архитектуру сервиса можно найти в описании задания 3. Для полноценного key-value хранилища нам не хватает рабочего сервера, который собственно будет отвечать на запросы клиента и хранить все данные. Интерфейс сервера мы придумали следующий:
*put(key, value)
*put_backup(key, value)
*get(key)
*tick()
В обязанности сервера входит посылать пинги координатору на каждый tick. В ответ он будет получать информацию о текущем view, в соответствии с которой он должен себя вести. Сервер, назначенный мастером, отвечает на запросы put и get от клиента и дублирует все запросы put на бекап. Сервер, назначенный бекапом, отвечает ошибкой на запросы от клиента, вместо этого он хранит все данные, полученные от мастера. Сервер, не участвующий в текущем view, не делает ничего.
Обращаю внимание на важную деталь: при изменении номера view (даже если ничего больше не поменялось) сервер, назначенный мастером, обязан скопировать все имеющиеся у него данные на бекап, и только после этого подтвердить свое участие (т.е. послать пинг с номером нового view).
Тесты можно найти в разделе ресурсы. Они покрывают лишь небольшую часть возможных проблем, поэтому можете смело добавлять свои тесты и присылать вместе с заданием.


